{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "NLP_Project_1_Medical_Terminology.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building an NLP model to classify medical terminology, over 200k samples\n"
   ],
   "metadata": {
    "id": "AsmqPk75HuVw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "Zs2noocQHzNW"
   },
   "execution_count": 132,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Getting the data\n",
    "\n",
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_jQ6w9HZeQF",
    "outputId": "d289e0aa-6c86-43ca-f647-beb248f553de",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reading the data from each file into a variable\n",
    "\n",
    "def readlines(filename):\n",
    "\n",
    "  with open(filename, 'r') as f:\n",
    "    return f.readlines()\n",
    "    "
   ],
   "metadata": {
    "id": "oUPjbcXOScsC"
   },
   "execution_count": 134,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Setting our unprepared data into variables\n",
    "\n",
    "train_data_unprepared = readlines('pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt')\n",
    "test_data_unprepared = readlines('pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt')\n",
    "valid_data_unprepared = readlines('pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt')"
   ],
   "metadata": {
    "id": "N_6wu80HUe7v"
   },
   "execution_count": 135,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# function to preprocess our data into a more convenient way to store it into a DataFrame\n",
    "\n",
    "def transform_Into_DataFrame(file_data):\n",
    "  text = ''\n",
    "  target = ''\n",
    "  total_lines = 0\n",
    "  line_number = 0\n",
    "  stoppingPos = 0\n",
    "  line_Nr = 0\n",
    "  data = []\n",
    "\n",
    "  for i, line in enumerate(file_data):\n",
    "    if line.startswith('\\n'):\n",
    "      continue\n",
    "    if line.startswith('###'):\n",
    "      line_Nr = 0\n",
    "      if line_number != 0:\n",
    "\n",
    "        for abstract in data[stoppingPos:]:\n",
    "          abstract['total_lines'] = total_lines\n",
    "        stoppingPos = line_number\n",
    "        total_lines = 0\n",
    "\n",
    "      continue\n",
    "    target = line.partition(\"\\t\")[0]\n",
    "    text = line.partition(\"\\t\")[2][:-1]\n",
    "    \n",
    "    data.append({'line_number': line_Nr , 'target':target, 'text':text, 'total_lines':0})\n",
    "    line_number+=1\n",
    "    line_Nr +=1\n",
    "    total_lines+=1\n",
    "    text = ''\n",
    "    target = ''\n",
    "\n",
    "  return data\n",
    "  "
   ],
   "metadata": {
    "id": "A2nD5lykpq7m"
   },
   "execution_count": 136,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Setting the variables\n",
    "\n",
    "train_data = transform_Into_DataFrame(train_data_unprepared)\n",
    "test_data = transform_Into_DataFrame(test_data_unprepared)\n",
    "valid_data = transform_Into_DataFrame(valid_data_unprepared)"
   ],
   "metadata": {
    "id": "o3KIQcdUs0xY"
   },
   "execution_count": 137,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Transforming the data into DataFrame\n",
    "\n",
    "train_data_df = pd.DataFrame(train_data)\n",
    "test_data_df = pd.DataFrame(test_data)\n",
    "valid_data_df = pd.DataFrame(valid_data)"
   ],
   "metadata": {
    "id": "Ss-9EJysibiS"
   },
   "execution_count": 138,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Preparing the X and y data independent and dependent variables\n",
    "\n",
    "train_labels = train_data_df['target']\n",
    "train_sentences = train_data_df['text']\n",
    "\n",
    "valid_labels = valid_data_df['target']\n",
    "valid_sentences = valid_data_df['text']\n",
    "\n",
    "test_labels = test_data_df['target']\n",
    "test_sentences = test_data_df['text']\n"
   ],
   "metadata": {
    "id": "GWkwNKowirF0"
   },
   "execution_count": 139,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Tokenising our data transforming the labels into numbers using OneHot\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot = OneHotEncoder(sparse=False)\n",
    "\n",
    "train_labels = onehot.fit_transform(train_labels.to_numpy().reshape(-1,1))\n",
    "valid_labels = onehot.fit_transform(valid_labels.to_numpy().reshape(-1,1))\n",
    "test_labels = onehot.fit_transform(test_labels.to_numpy().reshape(-1,1))"
   ],
   "metadata": {
    "id": "-sms7Vp-jfYs"
   },
   "execution_count": 140,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Tokenising the sentences using tensorflow experimental preprocessing function `TextVectorization()` function\n",
    "# turning the tokens(words) into numbers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "train_vectorizer = TextVectorization()\n",
    "train_vectorizer.adapt(train_sentences)\n"
   ],
   "metadata": {
    "id": "iYgEoYgTZlxk"
   },
   "execution_count": 141,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Preparing the embedded function later to be used when constructing the model\n",
    "# 68000 words to understand, and embedd a word with 512 values\n",
    "\n",
    "embedded = tf.keras.layers.Embedding(68000, 512)"
   ],
   "metadata": {
    "id": "41VdmkRAZ8LS"
   },
   "execution_count": 142,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Creating the model\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "X = train_vectorizer(inputs)\n",
    "X = embedded(X)\n",
    "\n",
    "# I will be using LSTM(Long-Short Term Memory) because of its ability to memorize patterns faster\n",
    "\n",
    "X = tf.keras.layers.LSTM(64)(X)\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax')(X)\n",
    "model = tf.keras.Model(inputs,outputs, name='model')"
   ],
   "metadata": {
    "id": "sjswC632axum"
   },
   "execution_count": 143,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Use the tensorflow datasets for faster computing later when fitting the model \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_sentences, valid_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels))\n",
    "\n",
    "# prefetching the data\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "id": "bcqdZGtlblmG"
   },
   "execution_count": 144,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Compiling the model using Adam as our optimizer\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "id": "mCV7UJx_d6MG"
   },
   "execution_count": 145,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Fitting the model to the data using only 10% of the data from our batches for faster computing and testing\n",
    "\n",
    "model.fit(train_dataset,\n",
    "            epochs=8,\n",
    "            steps_per_epoch = int(0.1*len(train_dataset)),\n",
    "            validation_data = valid_dataset,\n",
    "            validation_steps=int(0.1*len(valid_dataset)));"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQvB7EpEd7NG",
    "outputId": "0b0df21a-4911-4e92-c2dc-ca405a793413",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Reinstantiating the code to train it from 0\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "X = train_vectorizer(inputs)\n",
    "X = embedded(X)\n",
    "\n",
    "X = tf.keras.layers.LSTM(64)(X)\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax')(X)\n",
    "model = tf.keras.Model(inputs,outputs, name='model')"
   ],
   "metadata": {
    "id": "sxLBceoUiQGl"
   },
   "execution_count": 147,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Compiling the model \n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ],
   "metadata": {
    "id": "188adsTi8eZ3"
   },
   "execution_count": 148,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Setting a callback for Learning Rate\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10 ** (epoch/20))\n",
    "history = model.fit(train_dataset,\n",
    "            epochs=8,\n",
    "            steps_per_epoch = int(0.1*len(train_dataset)),\n",
    "            validation_data = valid_dataset,\n",
    "            validation_steps=int(0.1*len(valid_dataset)),\n",
    "            callbacks=[lr_callback])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lbb_sctgiTjv",
    "outputId": "71dc874a-e7e4-4ca7-da4a-34f61a186ea1",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting the learning rate vs. Loss in order to determine the right value\n",
    "\n",
    "lrs = 1e-4 * (10 ** (tf.range(8)/20))\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.semilogx(lrs, history.history['loss'])\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title('Learning rate vs. Loss');"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "3hbc_mPxiwLj",
    "outputId": "a9fc2ea7-f29f-457b-eee8-7678f5f8025e",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Testing our model predicions\n",
    "\n",
    "print('Sentence is : ', valid_sentences[0])\n",
    "pred = model.predict([valid_sentences[0]])\n",
    "index_res = tf.math.argmax(pred[0]).numpy()\n",
    "labels = ['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS']\n",
    "print('Predicted: ', labels[index_res])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BbsSo5FgjnzW",
    "outputId": "e24ffa6f-e485-4d88-b143-d76db1810909",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "3ec7Qz8O_TEE"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}